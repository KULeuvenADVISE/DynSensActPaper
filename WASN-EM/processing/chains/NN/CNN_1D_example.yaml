constants:
  CLASS_COUNT: 10
  CHANNELS:  NaN
  FEATURE_VECTOR_LENGTH: NaN
  INPUT_SEQUENCE_LENGTH: NaN
  CONVOLUTION_BORDER_MODE: valid
  ACTIVATION: relu
  DROPOUT: 0.2
  S: NaN # word size

chain:
  # Conv layer
  - layer_name: Conv1D
    config:
      input_shape: [CHANNELS, FEATURE_VECTOR_LENGTH, INPUT_SEQUENCE_LENGTH] # [channel, data, time]
      filters: 64
      kernel_size: 5
      strides: 1
      padding: CONVOLUTION_BORDER_MODE
      use_bias: 1
    memory: {parameters: 1, output: 1, output_save: True}
  # Batch normalization
  - layer_name: BatchNormalization
    config:
      axis: 1 # data_axis
    memory: {parameters: 1, output: 1, output_save: False}
    wordsize: S
  # Detection layer
  - layer_name: Activation
    config:
      activation: ACTIVATION
    memory: {parameters: 1, output: 1, output_save: False}
    wordsize: S
  # Pooling layer
  - layer_name: MaxPooling1D
    config:
      pool_size: 5
      strides: 5
    memory: {parameters: 1, output: 1, output_save: False}
    wordsize: S
  # Drop out layer
  - layer_name: Dropout
    config:
      rate: DROPOUT
    memory: {parameters: 1, output: 1, output_save: False}
    wordsize: S
  # Conv. layer 
  - layer_name: Conv1D
    config:
      filters: 128
      kernel_size: 5
      strides: 1
      padding: CONVOLUTION_BORDER_MODE
      use_bias: 1
    memory: {parameters: 1, output: 1, output_save: True}
    wordsize: S
  # Batch normalization
  - layer_name: BatchNormalization
    config:
      axis: 1 # data_axis
    memory: {parameters: 1, output: 1, output_save: False}
    wordsize: S
  # Detection layer
  - layer_name: Activation
    config:
      activation: ACTIVATION
    memory: {parameters: 1, output: 1, output_save: False}
    wordsize: S
  # Pooling layer
  - layer_name: GlobalMaxPooling
    memory: {parameters: 1, output: 1, output_save: False}
    wordsize: S
  # Drop out layer
  - layer_name: Dropout
    config:
      rate: DROPOUT
    memory: {parameters: 1, output: 1, output_save: False}
    wordsize: S
  # Fully connected layer
  - layer_name: Dense
    config:
      units: 128
      activation: ACTIVATION
    memory: {parameters: 1, output: 1, output_save: True}
    wordsize: S
  # Drop out
  - layer_name: Dropout
    config:
      rate: DROPOUT
    memory: {parameters: 1, output: 1, output_save: False}
    wordsize: S
  # Fully connected layer
  - layer_name: Dense
    config:
      units: 128
      activation: ACTIVATION
    memory: {parameters: 1, output: 1, output_save: True}
    wordsize: S
  # Drop out
  - layer_name: Dropout
    config:
      rate: DROPOUT
    memory: {parameters: 1, output: 1, output_save: False}
    wordsize: S
  # Fully connected layer
  - layer_name: Dense
    config:
      units: 128
      activation: ACTIVATION
    memory: {parameters: 1, output: 1, output_save: True}
    wordsize: S
  # Drop out
  - layer_name: Dropout
    config:
      rate: DROPOUT
    memory: {parameters: 1, output: 1, output_save: False}
    wordsize: S
  # Output layer
  - layer_name: Dense
    config:
      units: CLASS_COUNT
      activation: softmax
    memory: {parameters: 1, output: 1, output_save: True}
    wordsize: S